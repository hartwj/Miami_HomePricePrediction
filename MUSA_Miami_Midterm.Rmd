---
title: "MUSA508 Midterm: Miami Home Price Prediction Markdown"
author: "Julian Hartwell & Juliana Zhou"
date: "October 20, 2020"
output: 
  html_document:
    code_folding: hide
    toc: true
    toc_float:
      toc_collapsed: true
    toc_depth: 3
    number_sections: true
    theme: lumen
---

```{r setup, message=FALSE, warning=FALSE, include=FALSE}
knitr::opts_chunk$set(
	error = FALSE,
	message = FALSE,
	warning = FALSE,
	echo = FALSE
)
# --- Setup: Libraries ----
# Regex parsing package
library(tidyverse)
library(ggplot2)
library(sf)
library(sp)
library(rmarkdown)
library(kableExtra)
library(viridis)
library(tidycensus)
library(mapview)
library(lubridate)
library(ggcorrplot)
library(raster)
library(stringr)
library(stargazer)
library(ggpubr)
library(caret) 
library(mapview)
library(ggspatial)


library(rgeos)
library(spdep)
library(geosphere)
library(ckanr)
library(FNN)
library(geosphere)
library(grid)
library(gridExtra)
library(ggstance)
library(ggspatial)
library(jtools)     
library(broom)
library(tufte)    #excluding for now..weird errors
library(readr)

options(scipen = 999)
# --- Setup: Aesthetics ----
## Aesthetics
mapTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle=element_text(face="italic"),
    plot.caption=element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),axis.title = element_blank(),
    axis.text = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2)
  )
}

plotTheme <- function(base_size = 12) {
  theme(
    text = element_text( color = "black"),
    plot.title = element_text(size = 14,colour = "black"),
    plot.subtitle = element_text(face="italic"),
    plot.caption = element_text(hjust=0),
    axis.ticks = element_blank(),
    panel.background = element_blank(),
    panel.grid.major = element_line("grey80", size = 0.1),
    panel.grid.minor = element_blank(),
    panel.border = element_rect(colour = "black", fill=NA, size=2),
    strip.background = element_rect(fill = "grey80", color = "white"),
    strip.text = element_text(size=12),
    axis.title = element_text(size=12),
    axis.text = element_text(size=10),
    plot.background = element_blank(),
    legend.background = element_blank(),
    legend.title = element_text(colour = "black", face = "italic"),
    legend.text = element_text(colour = "black", face = "italic"),
    strip.text.x = element_text(size = 14)
  )
}

palette5 <- c("#25CB10", "#5AB60C", "#8FA108",   "#C48C04", "#FA7800")

qBr <- function(df, variable, rnd) {
  if (missing(rnd)) {
    as.character(quantile(round(df[[variable]],0),
                          c(.01,.2,.4,.6,.8), na.rm=T))
  } else if (rnd == FALSE | rnd == F) {
    as.character(formatC(quantile(df[[variable]]), digits = 3),
                 c(.01,.2,.4,.6,.8), na.rm=T)
  }
}

q5 <- function(variable) {as.factor(ntile(variable, 5))}

# --- Setup: Functions ----

nn_function <- function(measureFrom,measureTo,k) {
  measureFrom_Matrix <- as.matrix(measureFrom)
  measureTo_Matrix <- as.matrix(measureTo)
  nn <-   
    get.knnx(measureTo, measureFrom, k)$nn.dist
  output <-
    as.data.frame(nn) %>%
    rownames_to_column(var = "thisPoint") %>%
    gather(points, point_distance, V1:ncol(.)) %>%
    arrange(as.numeric(thisPoint)) %>%
    group_by(thisPoint) %>%
    dplyr::summarize(pointDistance = mean(point_distance)) %>%
    arrange(as.numeric(thisPoint)) %>% 
    dplyr::select(-thisPoint) %>%
    pull()
  
  return(output)  
}

multipleRingBuffer <- function(inputPolygon, maxDistance, interval) 
{
  #create a list of distances that we'll iterate through to create each ring
  distances <- seq(0, maxDistance, interval)
  #we'll start with the second value in that list - the first is '0'
  distancesCounter <- 2
  #total number of rings we're going to create
  numberOfRings <- floor(maxDistance / interval)
  #a counter of number of rings
  numberOfRingsCounter <- 1
  #initialize an otuput data frame (that is not an sf)
  allRings <- data.frame()
  
  #while number of rings  counteris less than the specified nubmer of rings
  while (numberOfRingsCounter <= numberOfRings) 
  {
    #if we're interested in a negative buffer and this is the first buffer
    #(ie. not distance = '0' in the distances list)
    if(distances[distancesCounter] < 0 & distancesCounter == 2)
    {
      #buffer the input by the first distance
      buffer1 <- st_buffer(inputPolygon, distances[distancesCounter])
      #different that buffer from the input polygon to get the first ring
      buffer1_ <- st_difference(inputPolygon, buffer1)
      #cast this sf as a polygon geometry type
      thisRing <- st_cast(buffer1_, "POLYGON")
      #take the last column which is 'geometry'
      thisRing <- as.data.frame(thisRing[,ncol(thisRing)])
      #add a new field, 'distance' so we know how far the distance is for a give ring
      thisRing$distance <- distances[distancesCounter]
    }
    
    
    #otherwise, if this is the second or more ring (and a negative buffer)
    else if(distances[distancesCounter] < 0 & distancesCounter > 2) 
    {
      #buffer by a specific distance
      buffer1 <- st_buffer(inputPolygon, distances[distancesCounter])
      #create the next smallest buffer
      buffer2 <- st_buffer(inputPolygon, distances[distancesCounter-1])
      #This can then be used to difference out a buffer running from 660 to 1320
      #This works because differencing 1320ft by 660ft = a buffer between 660 & 1320.
      #bc the area after 660ft in buffer2 = NA.
      thisRing <- st_difference(buffer2,buffer1)
      #cast as apolygon
      thisRing <- st_cast(thisRing, "POLYGON")
      #get the last field
      thisRing <- as.data.frame(thisRing$geometry)
      #create the distance field
      thisRing$distance <- distances[distancesCounter]
    }
    
    #Otherwise, if its a positive buffer
    else 
    {
      #Create a positive buffer
      buffer1 <- st_buffer(inputPolygon, distances[distancesCounter])
      #create a positive buffer that is one distance smaller. So if its the first buffer
      #distance, buffer1_ will = 0. 
      buffer1_ <- st_buffer(inputPolygon, distances[distancesCounter-1])
      #difference the two buffers
      thisRing <- st_difference(buffer1,buffer1_)
      #cast as a polygon
      thisRing <- st_cast(thisRing, "POLYGON")
      #geometry column as a data frame
      thisRing <- as.data.frame(thisRing[,ncol(thisRing)])
      #add teh distance
      thisRing$distance <- distances[distancesCounter]
    }  
    
    #rbind this ring to the rest of the rings
    allRings <- rbind(allRings, thisRing)
    #iterate the distance counter
    distancesCounter <- distancesCounter + 1
    #iterate the number of rings counter
    numberOfRingsCounter <- numberOfRingsCounter + 1
  }
  
  #convert the allRings data frame to an sf data frame
  allRings <- st_as_sf(allRings)
}


```

```{r message=FALSE, warning=FALSE, include=FALSE, echo=FALSE}

# --- Part 1: Data Wrangling ----
miamiHomes <- st_read("studentsData.geojson")
miamiHomes.sf <- miamiHomes %>% 
  st_as_sf(coords = c("Longitude", "Latitude"), crs = 4326, agr = "constant") %>%
  st_transform('ESRI:102658')

### Read in base map
miami.base <- 
  st_read("https://opendata.arcgis.com/datasets/5ece0745e24b4617a49f2e098df8117f_0.geojson") %>%
  st_transform('ESRI:102658') %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_union()


xmin = st_bbox(miami.base)[[1]]
ymin = st_bbox(miami.base)[[2]]
xmax = st_bbox(miami.base)[[3]]  
ymax = st_bbox(miami.base)[[4]]

bbox = c(xmin, ymin, xmax, ymax)

# Specify saleYear as integer
miamiHomes.sf$saleYear <- as.integer(miamiHomes.sf$saleYear)
miamiHomes.sf$ID <- seq.int(nrow(miamiHomes.sf))


### Read in shoreline shapefile
shoreline <- st_read('https://opendata.arcgis.com/datasets/58386199cc234518822e5f34f65eb713_0.geojson') %>% 
  st_transform('ESRI:102658')

# Clip shoreline by base map & transform to points
shoreline <- st_intersection(shoreline, miami.base)
shoreline.point <- st_cast(shoreline,"POINT") 


### 2018 - 5-Year ACS Data by Census Tract
tracts18 <- 
  get_acs(geography = "tract", variables = c("B25026_001E","B02001_002E",
                                             "B19013_001E","B25058_001E",
                                             "B06012_002E"), 
          year=2018, state= 12, county= 086, geometry=T, output="wide") %>%
  st_transform('ESRI:102658') %>%
  rename(TotalPop = B25026_001E, 
         Whites = B02001_002E,
         MedHHInc = B19013_001E, 
         MedRent = B25058_001E,
         TotalPoverty = B06012_002E) %>%
  dplyr::select(-NAME, -starts_with("B")) %>%
  mutate(pctWhite = ifelse(TotalPop > 0, Whites / TotalPop,0),
         pctPoverty = ifelse(TotalPop > 0, TotalPoverty / TotalPop, 0)) %>%
  dplyr::select(-Whites, -TotalPoverty) 

tracts18 <- tracts18[miami.base,]


### Middle School
midschool <- 
  st_read("https://opendata.arcgis.com/datasets/dd2719ff6105463187197165a9c8dd5c_0.geojson") %>%
  filter(CITY == "Miami Beach" | CITY == "Miami") %>% 
  rename(midschoolID = ID)%>%
  st_transform('ESRI:102658') 

midschool <- midschool[miami.base,]

# --- Part 2: Feature Engineering ----

### Parsing XF variables (i.e., Pool/Fence/Patio)
# Make all XF# text lowercase and combine into one string variable
miamiHomes.sf <- miamiHomes.sf %>%
  mutate(XF1 = tolower(XF1)) %>%
  mutate(XF2 = tolower(XF2)) %>%
  mutate(XF3 = tolower(XF3)) %>%
  mutate(XF_all = paste(XF1,XF2,XF3,sep = " "))

# Use stringr package to create patio, pool, and fence dummy variables
miamiHomes.sf <- miamiHomes.sf %>%
  mutate(Pool = as.integer(str_detect(XF_all,"pool"))) %>%
  mutate(Fence = as.integer(str_detect(XF_all,"fence"))) %>%
  mutate(Patio = as.integer(str_detect(XF_all,"patio")))


### Attach tract data to home prices
miamiHomes.sf <- st_join(miamiHomes.sf, tracts18, join = st_within)

### Create dummy variables for middle school using one hot encoding
miamiSchools.sf <- st_join(miamiHomes.sf, midschool, join = st_within)

sch_dmy.df <- miamiSchools.sf %>%
  st_drop_geometry() %>%
  dplyr::select(ID, NAME) 

dmy <- dummyVars(" ~ .", data = sch_dmy.df)
school.dummies<- data.frame(predict(dmy, newdata = sch_dmy.df))

# Join dummies to miamiHomes.sf
miamiHomes.sf <- inner_join(miamiHomes.sf, school.dummies, by = 'ID')

# Rename dummies
miamiHomes.sf <- miamiHomes.sf %>%
  rename(Brownsville.MS = contains("Brownsville.Middle")) %>%
  rename(CitrusGrove.MS = contains("Citrus.Grove.Middle")) %>%
  rename(JosedeDiego.MS = contains("de.Diego..Jose.Middle")) %>%
  rename(GeorgiaJA.MS = contains("Jones.Ayers..Georgia.Middle")) %>%
  rename(KinlochPk.MS = contains("Kinloch.Park.Middle")) %>%
  rename(Madison.MS = contains("Madison.Middle")) %>%
  rename(Nautilus.MS = contains("Nautilus.Middle")) %>%
  rename(Shenandoah.MS = contains("Shenandoah.Middle")) %>%
  rename(WestMiami.MS = contains("West.Miami.Middle")) 


### Calculating shoreline distance
# Creating distance to shore feature
miamiHomes.sf <- miamiHomes.sf %>%
  mutate(Shore1 = nn_function(st_coordinates(st_centroid(miamiHomes.sf)),
                              st_coordinates(st_centroid(shoreline.point)),1))

miamiHomes.sf$Shore.mile <- miamiHomes.sf$Shore1/5280

### Create home Age variable and clean miamiHomes.sf for exploratory analyses #Julian added medHHInc for a future problem
miamiHomesClean.sf <- miamiHomes.sf %>%
  mutate(Age = saleYear - YearBuilt) %>%
  dplyr::select(ID, Folio, SalePrice, Property.City,
                LotSize, Bed, Bath, Stories, Pool, Fence, Patio, ActualSqFt, 
                Age, toPredict, Shore1, GEOID, MedHHInc, TotalPop, MedRent, pctWhite, pctPoverty, 
                Brownsville.MS, CitrusGrove.MS, JosedeDiego.MS, GeorgiaJA.MS, 
                KinlochPk.MS, Madison.MS, Nautilus.MS, Shenandoah.MS, WestMiami.MS, geometry) 

OLSvars <- c('ID', 'Folio', 'SalePrice','Property.City',
            'LotSize','Bed','Bath','Stories','Pool','Fence','Patio', 'ActualSqFt',
            'Age','toPredict','Shore1','GEOID', 'MedHHInc', 'TotalPop', 'MedRent','pctWhite','pctPoverty','Brownsville.MS','CitrusGrove.MS','JosedeDiego.MS','GeorgiaJA.MS', 'KinlochPk.MS','Madison.MS','Nautilus.MS','Shenandoah.MS','WestMiami.MS','geometry')

# --- Part 3: Exploratory Analysis ----

## Runing a Correlation Matrix to find interesting variables
miamiHomes.train <- miamiHomesClean.sf %>% 
  filter(toPredict == 0) %>%
  filter(SalePrice <= 1000000)
miamiHomes.test <- miamiHomesClean.sf %>% 
  filter(toPredict == 1)

# cor.test(miamiHomes.train$ActualSqFt, miamiHomes.train$SalePrice, method = "pearson")
# Rest of the correlations in Data section

```

# Introduction & Summary

An accurate home price prediction algorithm can reduce volatility in the housing market and take into account existing factors that may not be reflected in a home’s previous selling prices (e.g., new roof, new shopping center, etc.) However, predictive algorithms can also be exceedingly difficult to perfect. A falsely high average estimate in a neighborhood might lead home sellers to list their homes at too high an asking price and dragging out the process of selling their home, thereby introducing friction into the housing market. A falsely low estimate may depress the value of what is oftentimes a homeowner’s most valuable asset.  

This project attempts to predict housing prices in metropolitan Miami by taking into consideration a home’s unique features (e.g., fence, patio) as well as considering local amenities and external features like schools, parks, and access to major roads. One interesting finding from this process is that a home's location in a middle school zone shows a positive relationship with home prices, but bot elementary or high school zone. 

To create our model, we converted our features of interest into variables that can be fed into an OLS regression model. We tested each featured for correlation with home sale prices and fine-tuned our model until we were able to minimize error.  

After testing and rejecting several features that did not deduce our prediction errors (e.g., distances to nearest park, major road, and middle school), we ultimately settled on the features (dependent variables) listed below. 
                        
1. Location/External Features
    * Property.City: Miami, Miami City
    * GEOID: Census tract code
    * Shore1: distance from shoreline (feet)
    * MedRent: median average rent by census tract
    * pctWhite: % residents who identify as White
    * pctPoverty: % residents below poverty line
    * 9 binary variables for middle school area: Brownsville, Citrus Grove, Jose de Diego, Georgia Jones-Ayer, Kinloch Park, Madison, Nautilus, Shenandoah, West Miami
2. Internal Features
    * LotSize: lot square footage
    * Age: years since home was built
    * Bed: number of bedrooms
    * Bath: number of bathrooms
    * Stories: number of bathrooms
    * Pool: extra pool feature (0/1)
    * Fence: extra fence feature (0/1)
    * Patio: extra patio feature (0/1)



# Data

## Dependent Variable Map: Home Prices

The map below shows the spatial distribution of home prices in Miami and Miami Beach. Darker points represent more expensive homes, with the deepest purple shade representing any home 1 million dollars or higher. Given the extreme range of home prices in Miami (max approx. 27 million dollars), we felt it necessary to collapse the outlier homes into the highest tier of home prices.

```{r home prices, echo=FALSE, message=FALSE, warning=FALSE}
trainprice.sf <- miamiHomesClean.sf %>% 
  filter(toPredict == 0)

trn <- ggplot() + 
  annotation_map_tile("cartolight") +
  geom_sf(data=miami.base, fill='transparent') + 
  geom_sf(data=trainprice.sf, aes(color=SalePrice),
          show.legend = "line", size = 1) + 
  labs(title = "Training Data Sale Prices",
       caption = "Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL.") +
  scale_color_viridis(option="C", 
                      name = "Home Price\n($0-$1M+)", 
                      limits=c(10000,1000000),
                      breaks=c(0, 250000, 500000, 750000, 1000000),
                      direction = -1,
                      begin = 0,
                      end = .95,
                      na.value = .95)
trn

```

## Independent Variable Map #2: Distance from Shore

Here we see the spatial relationship between home sale prices and distance from the shore. Unsurprisingly, as we move farther inland, home prices decrease.

``` {r shore, message=FALSE, warning=FALSE}
# Map of Distance to Shore
ggplot() + 
  annotation_map_tile("cartolight") +
  geom_sf(data=miami.base, fill='transparent') + 
  geom_sf(data=miamiHomes.sf, aes(colour=Shore.mile),
          show.legend = "line", size= 1) + 
  labs(title = "Distance from Shoreline",
       caption = "Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL.") +
  scale_colour_viridis(name = "Distance (miles)")

```

## Independent Variable Map #3: Middle Schools

This map shows the relationship between middle school attendance zones and home sale prices.

``` {r middle school, message=FALSE, warning=FALSE}

### Middle School Areas
# Map of Middle School Areas
ggplot() + 
  annotation_map_tile(type = 'cartolight', progress = 'none') +
  geom_sf(data = midschool, fill = "#d3e9ff", alpha =.4, color = "#498cd3", lwd = .8)+
  geom_sf(data=trainprice.sf, aes(color=SalePrice),
          show.legend = "line", size = 1) + 
  labs(title = "Middle School Districts & Home Prices",
       caption = "Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL.") +
  scale_color_viridis(option="C", 
                      name = "Home Price\n($0-$1M+)", 
                      limits=c(10000,1000000),
                      breaks=c(0, 250000, 500000, 750000, 1000000),
                      direction = -1,
                      na.value = .97) 
```

## Independent Variable Map #4: Percent White Residents

Below is a map of the percent of White residents in each Census tract in Miami and Miami Beach. As shown below, although having a higher percentage of White residence does not appear to be closely correlated with home price, the absence of White residents is clearly tied to a lower estimate of home price.

```{r white, echo=FALSE, message=FALSE, warning=FALSE}
colors <- c('#f1eef6', '#bdc9e1', '#74a9cf', '#2b8cbe', '#045a8d')
tracts18$pctWhite100 <- (tracts18$pctWhite)*100

ggplot()+
  annotation_map_tile("cartolight") +
  geom_sf(data=tracts18, aes(fill = q5(pctWhite100)), lwd = 0) +
  geom_sf(data=trainprice.sf, aes(color=SalePrice), size=1) + 
  labs(title = "% White Residents (2018)",
       caption = "Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL.") +
  mapTheme() + 
  scale_fill_manual(values = colors,
                    name= "% White Residents\n(Quintile Breaks)", 
                    labels = qBr(tracts18, "pctWhite100"),
                    guide = guide_legend(reverse = TRUE)) +
  scale_color_viridis(option="C", 
                      name = "Home Price\n($0-$1M+)", 
                      limits=c(10000,1000000),
                      breaks=c(0, 250000, 500000, 750000, 1000000),
                      direction = -1,
                      begin = 0,
                      end = .95,
                      na.value = .95)
```

## Excluded Variable of Interest: Major Roads

Several other features such as distance from major roads, parks, and location within elementary and high school attendance zones were tested, but did not prove to be relevant. As shown below, there appears to be little relationship between distance from major roads and home price.

```{r roads, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
### Calculating distance to major roads: 1/8 mile ring buffers
### Distance from Major Roads buffer

### Read in base map
miami.base <- 
  st_read("https://opendata.arcgis.com/datasets/5ece0745e24b4617a49f2e098df8117f_0.geojson") %>%
  st_transform('ESRI:102658') %>%
  filter(NAME == "MIAMI BEACH" | NAME == "MIAMI") %>%
  st_union()

# Creating buffers for distance to major roads
roads <- 
  st_read("https://opendata.arcgis.com/datasets/8bc234275dc749329c4e242abcfc5a0f_0.geojson") %>%
  filter(CLASS == c('1','2')) %>%
  st_transform('ESRI:102658') 

miamiRds <- roads[miami.base,]

# Create unioned buffer for major roads
miamiRds.buffer <- st_union(st_buffer(miamiRds, 1320)) %>%
  st_sf() %>%
  mutate(Legend = "Unioned Buffer")

miamiRds.buffer <- filter(miamiRds.buffer, Legend=="Unioned Buffer")

# Create 1/4 mile ring buffers
miami.rings <- multipleRingBuffer(miamiRds.buffer, 1320*8, 1320) %>%
  rename(road_dist = distance)
```

```{r major roads, echo=FALSE, message=FALSE, warning=FALSE}
# Plot to check ring buffers
ggplot() + 
  annotation_map_tile(type = 'cartolight', progress = 'none') +
  geom_sf(data = miami.rings, fill = "white", alpha = 0.3) +
  geom_sf(data=miamiRds, color = "black", size= 2) +
  geom_sf(data=trainprice.sf, aes(colour=SalePrice),
          show.legend = "line", size= 1) +
  labs(title = "Home Price & Distances from Major Roads", 
       subtitle = "1/8 Mile Ring Buffers from Roads",
       caption = "Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL.") +
  scale_color_viridis(option='C',
                      name = "Home Price\n($0-$1M+)", 
                      limits=c(10000,1000000),
                      breaks=c(0, 250000, 500000, 750000, 1000000),
                      direction = -1,
                      na.value = .97)
```

## Summary Statistics of Variables/Features
```{r summary stat, message=FALSE, warning=FALSE, include=FALSE}

#Summary Statistics Table
sumstat.df <- miamiHomes.train %>%
  st_drop_geometry() %>%
  dplyr::select(SalePrice, Property.City,GEOID, 
                LotSize, Age, Stories, Bed, Bath, Pool, Fence, Patio, 
                Shore1, MedRent, pctWhite, pctPoverty, 
                Brownsville.MS, CitrusGrove.MS, JosedeDiego.MS, GeorgiaJA.MS, 
                KinlochPk.MS, Madison.MS, Nautilus.MS, Shenandoah.MS, WestMiami.MS) 
```

```{r summ table, message=FALSE, warning=FALSE, results = "asis"}
stargazer(sumstat.df, title="Summary Statistics", type='html', 
          summary.stat = c('n','mean','sd','min','max'))
```

## Correlation 
Below is a correlation matrix, showing the relatedness of each numeric variable to every other. The red-bounded box shows each variable's correlation with sale price, our dependent variable. 

```{r correlation matrix, message=FALSE, warning=FALSE}
###Correlation Matrix
# Create a new subset of the training data with geometry dropped
numericVars <- miamiHomesClean.sf %>% 
  st_drop_geometry() %>%
  filter(toPredict == 0) %>%
  filter(SalePrice <= 1000000) %>%
  dplyr::select(-ID, -Folio) %>%
  select_if(is.numeric) %>% 
  na.omit() %>%
  rename(Shore_Distance = Shore1)

# Plot correlation matrix
ggcorrplot(
  round(cor(numericVars), 1), 
  p.mat = cor_pmat(numericVars),
  colors = c("#FA7800", "white", "#25CB10"),
  type="lower",
  insig = "blank") +  
  labs(title = "Correlation across numeric variables") +
  geom_rect(aes(xmin = 0, xmax = 24.5, ymin = 0.25, ymax = 1.75),
            fill = "transparent", color = "red", size = 1)

```

## Scatterplots

The below plots show the linear relationship between 4 independent variables, and home prices. Actual square footage is most highly and positively correlated with home price. Median rent in a home's area also has a small positive relationship, and distance from the shore and age are quite expectedly negatively correlated (i.e., as the age of a home increases, home price decreases).

```{r scatterplots, message=FALSE, warning=FALSE}
# Home price correlation scatterplots 
a <- ggplot(trainprice.sf, aes(y=SalePrice, x = ActualSqFt)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(title = "Price vs. SqFt (Actual)")+
  xlab(label = 'Square Footage')

b <- ggplot(trainprice.sf, aes(y=SalePrice, x = Shore1/5280)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(title = "Price vs. Shore Distance")+
  xlab(label = 'Distance (miles)')

c <- ggplot(trainprice.sf, aes(y=SalePrice, x = MedRent)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(title = "Price vs. Avg Rent (Tract)")+
  xlab(label='Median HHI')

d <- ggplot(trainprice.sf, aes(y=SalePrice, x = Age)) +
  geom_point() +
  geom_smooth(method = "lm")+
  labs(title = "Price vs. Home Age")+
  xlab(label='Age')

ggarrange(a,b,c,d, ncol = 2, nrow = 2)

```

# Methods

Our goal was to build a model that most accurately predicted Miami home prices. This was not a black-and-white procedure. We had three main considerations: 1) explain the variance in home prices, 2) minimize errors in predictions, and 3) be generalizable. Our first models used a simple ordinary least squares (OLS) regression. The results allowed us to exclude some census variables and remove overlapping variables (like square feet). In the next step we split our dataset 60/40, trained a model on 60% of the data, and tested it on the remaining 40%. This process gave us some insight into the model’s generalizability; it also allowed us to remove some variables. For example, this stage revealed that census tracts were our most influential neighborhood categorization, instead of property or mailing zip codes.

The last stage involved a machine learning method known as “k-fold cross validation”. Basically we split our dataset into ten different subsets, which were then divided further into training and test datasets. We measured the average performance across these folds - this was the best way to measure generalizability. We tried different combinations of features until we settled on a model that had the lowest errors in relation to actual sale price. 

 






```{r regression, message=FALSE, warning=FALSE, include=FALSE}


Reg1 <- lm(miamiHomes.train$SalePrice ~ ., data = miamiHomes.train %>%
             st_drop_geometry() %>%
  dplyr::select(Shore1, MedHHInc, TotalPop, MedRent, pctWhite, pctPoverty, 
                Brownsville.MS, CitrusGrove.MS, JosedeDiego.MS, GeorgiaJA.MS, 
                KinlochPk.MS, Madison.MS, Nautilus.MS, Shenandoah.MS, WestMiami.MS,))

Reg2 <- lm(miamiHomes.train$SalePrice ~ ., data = miamiHomes.train %>%
             st_drop_geometry()    %>%
             dplyr::select(-GEOID, -ID, -toPredict))

summary(Reg1)
summary(Reg2)
summ(Reg1)

```

```{r stargazer,  message=FALSE, warning=FALSE, results="asis"}
stargazer(Reg1, Reg2, title="Training Set LM Results", align=TRUE, type = "html", out = "Regression_Results.htm")


#GEOID R2 = .3, MailingZip =.4, PropertyZip =.9

```

```{r models, message=FALSE, warning=FALSE, include=FALSE}
# Same features as our 2nd model
# set random seed
set.seed(31711)

# get index for training sample
inTrain <- caret::createDataPartition(
  y = miamiHomes.train$SalePrice, 
  p = .60, list = FALSE)

# split data into training and test
miami.training <- miamiHomes.train[inTrain,] 
miami.test     <- miamiHomes.train[-inTrain,]  


reg2_split <- lm(SalePrice ~ ., data = miami.training %>%
                   st_drop_geometry() %>%
             dplyr::select(-GEOID, -ID, -toPredict))

summary(reg2_split)

```





# Results

Our first regression was based on our feature engineering. We included census variables, shoreline distance and middle school distance, but excluded household details. This resulted in several statistically significant features, an adjusted R2 of 0.599 and an overall model F-statistic implies statistical significance. The intercept, $274,600, is the estimated mean sale price when all of our independent variables are zero. For every additional foot from the shore, the sale price will drop by $5.75. Our second model below on the right, includes all of the census variables, household variables, and some of our feature engineering. The adjusted R2 improves to 0.732. 

This table shows our mean absolute error and mean absolute percent error for a single test set. Clearly more work needed to be done to improve our model.

```{r mae/mape, warning=FALSE, message=FALSE, echo=FALSE}
# Calculating MAE and MAPE for a single test test
mape <- function(actual,pred){
  mape <- mean(abs((actual - pred)/actual))*100
  return (mape)
}

reg2MAE <- caret::MAE(predict(reg2_split), miami.test$SalePrice)
reg2MAPE <- mape(miami.test$SalePrice,predict(reg2_split))


cat("Single Test MAE and MAPE"," \n","Test MAE: ", as.integer(reg2MAE), " \n","Test MAPE: ", as.integer(reg2MAPE)) %>%
  knitr::kable()
  

#The neighborhood variables clearly make our model better, but could still be overfitting
```

These are the high level results of our final model based on the features discussed in the introduction. We were able to explain about 75% of the variance in sale prices, while our distance from sale price average about +/- $66,000. 
```{r histogram, echo=FALSE, message=FALSE, warning=FALSE}

fitControl <- trainControl(method = "cv", 
                           number = 100,
                           # savePredictions differs from book
                           savePredictions = TRUE)

set.seed(73506)
#No neighborhoods R2=0.788, MAE=525,277

reg.cv2 <- 
  train(SalePrice ~ ., data = miamiHomes.train %>%
          st_drop_geometry() %>%
          dplyr::select(-ActualSqFt, -TotalPop, -ID, 
                        -MedHHInc, -toPredict), 
        method = "lm", 
        trControl = fitControl, 
        na.action = na.omit)

print(reg.cv2)

#Providing Results of Cross Validation Test
kable(reg.cv2$results)


```

```{r oof preds, message=FALSE, warning=FALSE, include=FALSE, paged.print=TRUE}
# Folio, Property.City, LotSize, Bed, Bath, Stories, Pool, Fence, Patio, ActualSqFt, Age, 
# Shore1, GEOID, MedRent, pctWhite, pctPoverty
# GEOID = .8337, property zip = .99, mailing zip 0.845

# How do we interpret the spread of values between the folds?
# extract predictions from CV object
cv_preds <- reg.cv2$pred

## Create dataset with "out of fold" predictions and original data
map_preds <- miamiHomes.test %>% 
  rowid_to_column(var = "rowIndex") %>% 
  left_join(cv_preds, by = "rowIndex") %>% 
  mutate(SalePrice.AbsError = abs(pred - SalePrice))

```

```{r output, eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#output to CSV

output_preds <- map_preds %>%
  dplyr::select(pred, Folio) %>%
  mutate(team_name = "Florid-API Keys") %>%
  rename(prediction = pred)

write.csv(output_preds, "Florid-API Keys.csv")

```


The histogram below examines our final cross validation model run on 100 folds. The MAE, or Mean Absolute Error, measures the difference in actual and predicted sales price across each test subset. Our errors clustered between $50k and $75k. From this plot alone, it is difficult to say whether our model is generalizable.
```{r mae histogram, message=FALSE, warning=FALSE, echo=FALSE}
#Creating an MAE Histogram
MAE_hist <- as.data.frame(reg.cv2$pred) %>%
  mutate(ABS_error = abs(pred-obs)) %>%
  group_by(Resample) %>%
  summarize(MAE = mean(ABS_error, na.rm = T))


ggplot(MAE_hist, aes(x=MAE))+geom_histogram()+
  labs(title="Histogram of Cross Validation MAE",
       caption="Based on 100 Folds")+
  theme_classic()
  

#Plotting Predicted Prices as a Function of Observed Prices

regCV2plot <- as.data.frame(reg.cv2$pred)


```

## Scatterplot of Predicted and Observed Sale Prices
Our final model was much more accurate at predicting prices that fell in the lower-middle range for homes. Because we removed homes that cost more than $1 million from our training data, our model is particularly nongeneralizable when it comes to the upper end of the price spectrum (vastly underestimates the price of expensive homes). Removing outliers improves our overall mean errors, but hurts our predictive power at higher home values. 

```{r pred v obs plot, message=FALSE, warning=FALSE}
ggplot(regCV2plot,aes(obs,pred)) +
  geom_point() +
  stat_smooth(aes(obs, obs), 
              method = "lm", se = FALSE, size = 1, colour="#FA7800") + 
  stat_smooth(aes(obs, pred), 
              method = "lm", se = FALSE, size = 1, colour="#25CB10") +
  labs(title="Predicted sale price as a function of observed price",
       subtitle="Orange line represents a perfect prediction; Green line represents prediction",
       x="Sale Price",
       y="Predicted Price") +
  plotTheme() + theme(plot.title = element_text(size = 18, colour = "black")) 

```

The following two maps show our predicted home prices based on our final model in comparison to all the actual home prices used in the training dataset. Our model fails to capture higher-priced homes along the coast and lower-priced homes in the Northwest by Hialeah (missing variation in colors as seen in training data map).

``` {r pred v obs map, echo=FALSE, message=FALSE, warning=FALSE}
allprices.sf <- full_join(trainprice.sf %>% as.data.frame(), map_preds %>% as.data.frame())%>%
  st_sf()

# Writing predictions into SalePrice variable for mapping
allprices.sf$SalePrice[allprices.sf$SalePrice == 0] <- NA   
allprices.sf$SalePrice <- ifelse(is.na(allprices.sf$SalePrice), 
                                 allprices.sf$pred, allprices.sf$SalePrice)

# Map of Predicted Home Prices vs. Training
tst <- ggplot() + 
  annotation_map_tile("cartolight") +
  geom_sf(data=miami.base, fill='transparent') + 
  geom_sf(data=map_preds, aes(color=pred),
          show.legend = "line", size = 1) + 
  labs(title = "Predicted Home Prices") +
  scale_color_viridis(option="C", 
                      name = "Price ($)", 
                      limits=c(10000,1000000),
                      breaks=c(0, 250000, 500000, 750000, 1000000),
                      direction = -1,
                      begin = 0,
                      end = .95,
                      na.value = .95)
ggarrange(tst, trn, ncol = 2, nrow = 1, 
          common.legend = TRUE, legend = "right")
```

```{r spatial corr of errors, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
# Is there a spatial correlation of errors?

# get index for training sample
inTrain <- caret::createDataPartition(
  y = miamiHomes.train$SalePrice, 
  p = .60, list = FALSE)
# split data into training and test
miami.training <- miamiHomes.train[inTrain,] 
miami.test     <- miamiHomes.train[-inTrain,]  

#this is the dataset and variables we used in regCV2
#however I had to drop GEOID since there was a levels error
finaltraining <- lm(SalePrice ~ ., data = miami.training %>%
                      st_drop_geometry() %>%
                   dplyr::select(-GEOID, -ID, 
                                 -TotalPop, -MedHHInc, -toPredict))

miami.test <-
  miami.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(finaltraining, miami.test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)



k_nearest_neighbors = 5
#prices
coords <- st_coordinates(st_centroid(miamiHomesClean.sf))
# k nearest neighbors
neighborList <- knn2nb(knearneigh(coords, k_nearest_neighbors))
spatialWeights <- nb2listw(neighborList, style="W")
miamiHomesClean.sf$lagPrice <- lag.listw(spatialWeights, miamiHomesClean.sf$SalePrice)

miami.test.lag <- drop_na(miami.test)
#errors
coords.test <- st_coordinates(st_centroid(miami.test.lag))
neighborList.test <- knn2nb(knearneigh(coords.test, k_nearest_neighbors))
spatialWeights.test <- nb2listw(neighborList.test, style="W")
miami.test.lag$lagPriceError <- lag.listw(spatialWeights.test, miami.test.lag$SalePrice.AbsError)
```

Home prices are spatially autocorrelated with neighbors according to our model. In addition, the bottom chart shows that errors are also spacially correlated with neighbors. 

```{r, spatial lag err plots, echo=FALSE, message=FALSE, warning=FALSE}
# Spatial Lag Error Plots
ggplot(miamiHomesClean.sf, aes(x=lagPrice, y=SalePrice)) +
  geom_point(colour = "#FA7800") +
  geom_smooth(method = "lm", se = FALSE, colour = "#25CB10") +
  labs(title = "Price as a function of the spatial lag of price",
       x = "Spatial lag of price (Mean price of 5 nearest neighbors)",
       y = "Sale Price") +
  plotTheme()

ggplot(miami.test.lag, aes(x=lagPriceError, y=SalePrice)) +
  geom_point(colour = "#FA7800") +
  geom_smooth(method = "lm", se = FALSE, colour = "#25CB10") +
  labs(title = "Error as a function of the spatial lag of price",
       caption = "",
       x = "Spatial lag of errors (Mean error of 5 nearest neighbors)",
       y = "Sale Price") +
  plotTheme()

```

Our observed Moran's I, denoted by the vertical orange line, is statistically higher than the randomly permuted I, thus there exists spatial autocorrelation in our final model. 

```{r morans, echo=FALSE, message=FALSE, warning=FALSE}
#Moran's I Test
moranTest <- moran.mc(miami.test.lag$SalePrice.AbsError, 
                      spatialWeights.test, nsim = 999, na.action=na.exclude)

ggplot(as.data.frame(moranTest$res[c(1:999)]), aes(moranTest$res[c(1:999)])) +
  geom_histogram(binwidth = 0.01) +
  geom_vline(aes(xintercept = moranTest$statistic), colour = "#FA7800",size=1) +
  scale_x_continuous(limits = c(-1, 1)) +
  labs(title="Observed and permuted Moran's I",
       subtitle= "Observed Moran's I in orange",
       x="Moran's I",
       y="Count") +
  plotTheme()

```

The chart below looks at all 3503 observations from the dataset, including those where no sale price is known. It applies our final model predictions. 
```{r Predicted Values when toPredict is 0 and 1}
#Updated this, hopefully it works

PredictMap <-
  miamiHomesClean.sf %>%
  mutate(SalePrice.Predict = predict(finaltraining, miamiHomesClean.sf))

#This isn't working for me b/c R saying you need 6 colors but palette5 only has 5?
#But don't think we need it, I already made a mape of the predicted prices (in that ggarrange plot)
#ggplot() +
  #annotation_map_tile("cartolight") +
  #geom_sf(data=miami.base, aes(fill='transparent')) +
  #geom_sf(data = PredictMap, aes(fill = q5(SalePrice.Predict))) +
  #labs(title="Precited Prices of All Homes") +
  #scale_fill_manual(values = palette5,
                    #labels=qBr(PredictMap,"SalePrice.Predict"),
                    #name="Quintile\nBreaks") +
  #mapTheme() 


ggplot() + 
  annotation_map_tile("cartolight") +
  geom_sf(data=miami.base, fill='transparent') + 
  geom_sf(data=PredictMap, aes(color=SalePrice.Predict),
          show.legend = "line", size = 1) + 
  labs(title = "Predicted Home Prices") +
  scale_color_viridis(option="C", 
                      name = "Price ($)", 
                      limits=c(10000,1000000),
                      breaks=c(0, 250000, 500000, 750000, 1000000),
                      direction = -1,
                      begin = 0,
                      end = .95,
                      na.value = .95)



```
### Commented out some plots b/c don't think they're needed (See Below)
```{r mape map, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#i think we can delete this code
# MAPE by Neighborhood

#nhood_sum %>% 
  #st_drop_geometry %>%
  #arrange(desc(meanMAE)) %>% 
  #kable() %>% kable_styling()



```
These next two maps look at our residuals and test set mean errors by neighborhood. 
````{r Residuals and MAPE by Neighborhood, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
miami.test <-
  miami.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(finaltraining, miami.test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)

map_preds_sum <- map_preds %>% 
  group_by(GEOID) %>% 
  summarise(meanMAPE = mean(SalePrice.APE))
#if this doesn't work remove annotation map

ggplot() +
  annotation_map_tile("cartolight") +
  geom_sf(data=miami.base, fill='transparent') + 
  geom_sf(data = miami.test, aes(color = SalePrice.Error, size = .1)) +
  scale_fill_manual(values = palette5,
                    labels=qBr(miami.test,"SalePrice.Error"),
                    name="Quintile\nBreaks")
  labs(title = "Test Set Residuals")+
  mapTheme()


ggplot() +
  annotation_map_tile("cartolight") +
  geom_sf(data=miami.base, fill='transparent') + 
  geom_sf(data = miamiHomesClean.sf %>% 
            left_join(st_drop_geometry(map_preds_sum), by = "GEOID"),
          aes(color = q5(meanMAE)), size = .8) +
  scale_fill_manual(values = palette5,
                    labels=qBr(nhood_sum,"meanMAE"),
                    name="Quintile\nBreaks") +
  mapTheme() +
  labs(title="Absolute sale price errors on the out-of-fold set by Neighborhood",
       caption = "Map tiles by Carto, under CC BY 3.0. Data by OpenStreetMap, under ODbL.")
  

```
This scatterplot looks at our errors in relation to price by neighborhood. Our errors grow larger for more expensive homes. 
````{r Price-MAPE Scatter, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
miami.test <-
  miami.test %>%
  mutate(Regression = "Baseline Regression",
         SalePrice.Predict = predict(finaltraining, miami.test),
         SalePrice.Error = SalePrice.Predict - SalePrice,
         SalePrice.AbsError = abs(SalePrice.Predict - SalePrice),
         SalePrice.APE = (abs(SalePrice.Predict - SalePrice)) / SalePrice.Predict)


#Scatter of MAPE & Mean Price by Neighborhood
nhood_sum <- miami.test %>% 
  group_by(GEOID) %>%
  summarize(meanPrice = mean(SalePrice, na.rm = T),
            meanPrediction = mean(SalePrice.Predict, na.rm = T),
            meanMAE = mean(SalePrice.AbsError, na.rm = T))

plot(nhood_sum$meanPrice, nhood_sum$meanMAE, 
     main = "Scatter of Mean Price and Mean MAE by Census Tract", 
     xlab = "Mean Price",
     ylab = "Mean MAE")

```
<<<<<<< HEAD

## Split by Census Group: Generalizability by Income
=======
>>>>>>> 9581a3c850267e35298e2c8cb0298cf2abb49bf1
These results show that our model does differ drastically across income, therefore struggling with generalizability.
```{r income generalizability, echo=FALSE, message=FALSE, warning=FALSE, include=FALSE}
#Testing Generalizability of Median Household Income Split

median(as.numeric(miami.test$MedHHInc), na.rm=TRUE)

miamiPoor <- miami.test %>% 
  filter(MedHHInc<= 37117)
miamiRich <- miami.test %>% 
  filter(MedHHInc> 37117)

miamiPoorLM <- 
  train(SalePrice ~ ., data = miamiPoor %>%
          st_drop_geometry() %>%
          dplyr::select(-ID, -Property.City, -ActualSqFt, -TotalPop, 
                        -MedHHInc, -toPredict,
                        -Regression, -SalePrice.Predict, 
                        -SalePrice.Error, -SalePrice.AbsError,
                        -SalePrice.APE), 
        method = "lm", 
        trControl = fitControl, 
        na.action = na.pass)


miamiRichLM <- 
  train(SalePrice ~ ., data = miamiRich %>%
          st_drop_geometry() %>%
          dplyr::select(-ActualSqFt, -TotalPop, 
                        -MedHHInc, -toPredict,
                        -Regression, -SalePrice.Predict, -SalePrice.Error, -SalePrice.AbsError, -SalePrice.APE), 
        method = "lm", 
        trControl = fitControl, 
        na.action = na.omit)

```

```{r}
kable(miamiPoorLM$results)

kable(miamiRichLM$results)
```
